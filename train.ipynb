{"cells":[{"cell_type":"markdown","metadata":{"id":"afAL_jTeufdz"},"source":["# **Training**\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6944,"status":"ok","timestamp":1724268911319,"user":{"displayName":"Boolean Orbs","userId":"07392357175196698456"},"user_tz":-360},"id":"jkjuMDN4rco9","outputId":"9d48b044-d3bd-4769-d093-ea3a7789d292"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","\n","drive.mount('/content/gdrive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2rXOEA9FriuL"},"outputs":[],"source":["ROOT_DIR = '/content/gdrive/My Drive/Model_train/data'"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":6237,"status":"ok","timestamp":1724268937632,"user":{"displayName":"Boolean Orbs","userId":"07392357175196698456"},"user_tz":-360},"id":"BpLovltfsLF_","outputId":"2417f9e3-6065-4784-eaf5-de7b2b7a2062"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: ultralytics in /usr/local/lib/python3.10/dist-packages (8.2.79)\n","Requirement already satisfied: numpy<2.0.0,>=1.23.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.26.4)\n","Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (3.7.1)\n","Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.10.0.84)\n","Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (9.4.0)\n","Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (6.0.2)\n","Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.32.3)\n","Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.13.1)\n","Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.3.1+cu121)\n","Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.18.1+cu121)\n","Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.66.5)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from ultralytics) (5.9.5)\n","Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from ultralytics) (9.0.0)\n","Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.1.4)\n","Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.13.1)\n","Requirement already satisfied: ultralytics-thop>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.0.5)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.2.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.53.1)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (24.1)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.1.2)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->ultralytics) (2024.1)\n","Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->ultralytics) (2024.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2024.7.4)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.15.4)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (4.12.2)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (1.13.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.1.4)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (2024.6.1)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (12.1.105)\n","Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (8.9.2.26)\n","Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (12.1.3.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (11.0.2.54)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (10.3.2.106)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (11.4.5.107)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (12.1.0.106)\n","Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (2.20.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (12.1.105)\n","Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (2.3.1)\n","Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.8.0->ultralytics) (12.6.20)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.16.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (2.1.5)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.8.0->ultralytics) (1.3.0)\n"]}],"source":["!pip install ultralytics"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":17524743,"status":"ok","timestamp":1724205273680,"user":{"displayName":"Boolean Orbs","userId":"07392357175196698456"},"user_tz":-360},"id":"JKFAxTMytFmd","outputId":"fcad967c-2375-411a-f8ea-27f44bc51f0d"},"outputs":[{"name":"stdout","output_type":"stream","text":["Ultralytics YOLOv8.2.79 🚀 Python-3.10.12 torch-2.3.1+cu121 CPU (Intel Xeon 2.20GHz)\n","\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=yolov8n.pt, data=/content/gdrive/My Drive/Model_train/data/config.yaml, epochs=40, time=None, patience=100, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=train2, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs/detect/train2\n","Overriding model.yaml nc=80 with nc=4\n","\n","                   from  n    params  module                                       arguments                     \n","  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n","  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n","  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n","  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n","  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n","  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n","  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n","  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n","  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n","  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n"," 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n"," 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n"," 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n"," 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n"," 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n"," 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n"," 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n"," 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n"," 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n"," 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n"," 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n"," 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n"," 22        [15, 18, 21]  1    752092  ultralytics.nn.modules.head.Detect           [4, [64, 128, 256]]           \n","Model summary: 225 layers, 3,011,628 parameters, 3,011,612 gradients, 8.2 GFLOPs\n","\n","Transferred 319/355 items from pretrained weights\n","\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/detect/train2', view at http://localhost:6006/\n","Freezing layer 'model.22.dfl.conv.weight'\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/gdrive/My Drive/Model_train/data/labels/train.cache... 421 images, 1 backgrounds, 1 corrupt: 100%|██████████| 421/421 [00:00<?, ?it/s]"]},{"name":"stdout","output_type":"stream","text":["\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/gdrive/My Drive/Model_train/data/images/train/hole (10).jpg: corrupt JPEG restored and saved\n","\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/gdrive/My Drive/Model_train/data/images/train/hole (100).jpg: corrupt JPEG restored and saved\n","\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/gdrive/My Drive/Model_train/data/images/train/hole (101).jpg: corrupt JPEG restored and saved\n","\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/gdrive/My Drive/Model_train/data/images/train/hole (102).jpg: corrupt JPEG restored and saved\n","\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/gdrive/My Drive/Model_train/data/images/train/hole (103).jpg: corrupt JPEG restored and saved\n","\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/gdrive/My Drive/Model_train/data/images/train/hole (11).jpg: corrupt JPEG restored and saved\n","\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/gdrive/My Drive/Model_train/data/images/train/hole (17).jpg: corrupt JPEG restored and saved\n","\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/gdrive/My Drive/Model_train/data/images/train/hole (18).jpg: corrupt JPEG restored and saved\n","\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/gdrive/My Drive/Model_train/data/images/train/hole (19).jpg: corrupt JPEG restored and saved\n","\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/gdrive/My Drive/Model_train/data/images/train/hole (20).jpg: corrupt JPEG restored and saved\n","\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/gdrive/My Drive/Model_train/data/images/train/hole (21).jpg: corrupt JPEG restored and saved\n","\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/gdrive/My Drive/Model_train/data/images/train/hole (22).jpg: corrupt JPEG restored and saved\n","\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/gdrive/My Drive/Model_train/data/images/train/hole (47).jpg: corrupt JPEG restored and saved\n","\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/gdrive/My Drive/Model_train/data/images/train/hole (48).jpg: corrupt JPEG restored and saved\n","\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/gdrive/My Drive/Model_train/data/images/train/hole (49).jpg: corrupt JPEG restored and saved\n","\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/gdrive/My Drive/Model_train/data/images/train/hole (50).jpg: corrupt JPEG restored and saved\n","\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/gdrive/My Drive/Model_train/data/images/train/hole (51).jpg: corrupt JPEG restored and saved\n","\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/gdrive/My Drive/Model_train/data/images/train/hole (52).jpg: corrupt JPEG restored and saved\n","\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/gdrive/My Drive/Model_train/data/images/train/hole (53).jpg: corrupt JPEG restored and saved\n","\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/gdrive/My Drive/Model_train/data/images/train/hole (54).jpg: corrupt JPEG restored and saved\n","\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/gdrive/My Drive/Model_train/data/images/train/hole (55).jpg: corrupt JPEG restored and saved\n","\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/gdrive/My Drive/Model_train/data/images/train/hole (56).jpg: corrupt JPEG restored and saved\n","\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/gdrive/My Drive/Model_train/data/images/train/hole (57).jpg: corrupt JPEG restored and saved\n","\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/gdrive/My Drive/Model_train/data/images/train/hole (58).jpg: corrupt JPEG restored and saved\n","\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/gdrive/My Drive/Model_train/data/images/train/hole (59).jpg: corrupt JPEG restored and saved\n","\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/gdrive/My Drive/Model_train/data/images/train/hole (60).jpg: corrupt JPEG restored and saved\n","\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/gdrive/My Drive/Model_train/data/images/train/hole (61).jpg: corrupt JPEG restored and saved\n","\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/gdrive/My Drive/Model_train/data/images/train/hole (62).jpg: corrupt JPEG restored and saved\n","\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/gdrive/My Drive/Model_train/data/images/train/hole (63).jpg: corrupt JPEG restored and saved\n","\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/gdrive/My Drive/Model_train/data/images/train/hole (64).jpg: corrupt JPEG restored and saved\n","\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/gdrive/My Drive/Model_train/data/images/train/hole (65).jpg: corrupt JPEG restored and saved\n","\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/gdrive/My Drive/Model_train/data/images/train/hole (66).jpg: corrupt JPEG restored and saved\n","\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/gdrive/My Drive/Model_train/data/images/train/hole (67).jpg: corrupt JPEG restored and saved\n","\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/gdrive/My Drive/Model_train/data/images/train/hole (68).jpg: corrupt JPEG restored and saved\n","\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/gdrive/My Drive/Model_train/data/images/train/hole (69).jpg: corrupt JPEG restored and saved\n","\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/gdrive/My Drive/Model_train/data/images/train/hole (81).jpg: corrupt JPEG restored and saved\n","\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/gdrive/My Drive/Model_train/data/images/train/hole (82).jpg: corrupt JPEG restored and saved\n","\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/gdrive/My Drive/Model_train/data/images/train/hole (83).jpg: corrupt JPEG restored and saved\n","\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/gdrive/My Drive/Model_train/data/images/train/hole (84).jpg: corrupt JPEG restored and saved\n","\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/gdrive/My Drive/Model_train/data/images/train/hole (85).jpg: corrupt JPEG restored and saved\n","\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/gdrive/My Drive/Model_train/data/images/train/hole (86).jpg: corrupt JPEG restored and saved\n","\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/gdrive/My Drive/Model_train/data/images/train/hole (87).jpg: corrupt JPEG restored and saved\n","\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/gdrive/My Drive/Model_train/data/images/train/hole (88).jpg: corrupt JPEG restored and saved\n","\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/gdrive/My Drive/Model_train/data/images/train/hole (89).jpg: corrupt JPEG restored and saved\n","\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/gdrive/My Drive/Model_train/data/images/train/hole (9).jpg: corrupt JPEG restored and saved\n","\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/gdrive/My Drive/Model_train/data/images/train/hole (90).jpg: corrupt JPEG restored and saved\n","\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/gdrive/My Drive/Model_train/data/images/train/hole (91).jpg: corrupt JPEG restored and saved\n","\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/gdrive/My Drive/Model_train/data/images/train/hole (92).jpg: corrupt JPEG restored and saved\n","\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/gdrive/My Drive/Model_train/data/images/train/hole (93).jpg: corrupt JPEG restored and saved\n","\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/gdrive/My Drive/Model_train/data/images/train/hole (94).jpg: corrupt JPEG restored and saved\n","\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/gdrive/My Drive/Model_train/data/images/train/hole (95).jpg: corrupt JPEG restored and saved\n","\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/gdrive/My Drive/Model_train/data/images/train/hole (96).jpg: corrupt JPEG restored and saved\n","\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/gdrive/My Drive/Model_train/data/images/train/hole (97).jpg: corrupt JPEG restored and saved\n","\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/gdrive/My Drive/Model_train/data/images/train/hole (98).jpg: corrupt JPEG restored and saved\n","\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/gdrive/My Drive/Model_train/data/images/train/hole (99).jpg: corrupt JPEG restored and saved\n","\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/gdrive/My Drive/Model_train/data/images/train/loop (36).jpg: corrupt JPEG restored and saved\n","\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/gdrive/My Drive/Model_train/data/images/train/loop (37).jpg: corrupt JPEG restored and saved\n","\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/gdrive/My Drive/Model_train/data/images/train/loop (38).jpg: corrupt JPEG restored and saved\n","\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/gdrive/My Drive/Model_train/data/images/train/loop (40).jpg: corrupt JPEG restored and saved\n","\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/gdrive/My Drive/Model_train/data/images/train/loop (69).jpg: corrupt JPEG restored and saved\n","\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/gdrive/My Drive/Model_train/data/images/train/setup (10).jpg: corrupt JPEG restored and saved\n","\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/gdrive/My Drive/Model_train/data/images/train/setup (11).jpg: corrupt JPEG restored and saved\n","\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/gdrive/My Drive/Model_train/data/images/train/setup (12).jpg: corrupt JPEG restored and saved\n","\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/gdrive/My Drive/Model_train/data/images/train/setup (13).jpg: corrupt JPEG restored and saved\n","\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/gdrive/My Drive/Model_train/data/images/train/setup (14).jpg: corrupt JPEG restored and saved\n","\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/gdrive/My Drive/Model_train/data/images/train/setup (15).jpg: corrupt JPEG restored and saved\n","\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/gdrive/My Drive/Model_train/data/images/train/setup (16).jpg: corrupt JPEG restored and saved\n","\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/gdrive/My Drive/Model_train/data/images/train/setup (17).jpg: corrupt JPEG restored and saved\n","\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/gdrive/My Drive/Model_train/data/images/train/setup (5).jpg: corrupt JPEG restored and saved\n","\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/gdrive/My Drive/Model_train/data/images/train/stain (2).jpg: ignoring corrupt image/label: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mval: \u001b[0mScanning /content/gdrive/My Drive/Model_train/data/labels/val.cache... 149 images, 0 backgrounds, 0 corrupt: 100%|██████████| 149/149 [00:00<?, ?it/s]"]},{"name":"stdout","output_type":"stream","text":["\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /content/gdrive/My Drive/Model_train/data/images/val/hole (12).jpg: corrupt JPEG restored and saved\n","\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /content/gdrive/My Drive/Model_train/data/images/val/hole (13).jpg: corrupt JPEG restored and saved\n","\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /content/gdrive/My Drive/Model_train/data/images/val/hole (14).jpg: corrupt JPEG restored and saved\n","\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /content/gdrive/My Drive/Model_train/data/images/val/hole (15).jpg: corrupt JPEG restored and saved\n","\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /content/gdrive/My Drive/Model_train/data/images/val/hole (16).jpg: corrupt JPEG restored and saved\n","\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /content/gdrive/My Drive/Model_train/data/images/val/hole (70).jpg: corrupt JPEG restored and saved\n","\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /content/gdrive/My Drive/Model_train/data/images/val/hole (71).jpg: corrupt JPEG restored and saved\n","\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /content/gdrive/My Drive/Model_train/data/images/val/hole (72).jpg: corrupt JPEG restored and saved\n","\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /content/gdrive/My Drive/Model_train/data/images/val/hole (73).jpg: corrupt JPEG restored and saved\n","\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /content/gdrive/My Drive/Model_train/data/images/val/hole (74).jpg: corrupt JPEG restored and saved\n","\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /content/gdrive/My Drive/Model_train/data/images/val/hole (75).jpg: corrupt JPEG restored and saved\n","\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /content/gdrive/My Drive/Model_train/data/images/val/hole (76).jpg: corrupt JPEG restored and saved\n","\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /content/gdrive/My Drive/Model_train/data/images/val/hole (77).jpg: corrupt JPEG restored and saved\n","\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /content/gdrive/My Drive/Model_train/data/images/val/hole (78).jpg: corrupt JPEG restored and saved\n","\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /content/gdrive/My Drive/Model_train/data/images/val/hole (79).jpg: corrupt JPEG restored and saved\n","\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /content/gdrive/My Drive/Model_train/data/images/val/hole (80).jpg: corrupt JPEG restored and saved\n","\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /content/gdrive/My Drive/Model_train/data/images/val/setup (6).jpg: corrupt JPEG restored and saved\n","\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /content/gdrive/My Drive/Model_train/data/images/val/setup (7).jpg: corrupt JPEG restored and saved\n","\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /content/gdrive/My Drive/Model_train/data/images/val/setup (8).jpg: corrupt JPEG restored and saved\n","\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /content/gdrive/My Drive/Model_train/data/images/val/setup (9).jpg: corrupt JPEG restored and saved\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["Plotting labels to runs/detect/train2/labels.jpg... \n","\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n","\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.00125, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n","\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added ✅\n","Image sizes 640 train, 640 val\n","Using 0 dataloader workers\n","Logging results to \u001b[1mruns/detect/train2\u001b[0m\n","Starting training for 40 epochs...\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"name":"stderr","output_type":"stream","text":["       1/40         0G      1.649      3.555       1.49         15        640: 100%|██████████| 27/27 [07:26<00:00, 16.55s/it]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 5/5 [03:35<00:00, 43.07s/it]"]},{"name":"stdout","output_type":"stream","text":["                   all        149        295    0.00653      0.512      0.135      0.094\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"name":"stderr","output_type":"stream","text":["       2/40         0G      1.487      2.504      1.388          7        640: 100%|██████████| 27/27 [06:31<00:00, 14.51s/it]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 5/5 [00:45<00:00,  9.07s/it]"]},{"name":"stdout","output_type":"stream","text":["                   all        149        295    0.00617      0.481      0.179      0.141\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"name":"stderr","output_type":"stream","text":["       3/40         0G       1.57      2.279      1.433         25        640: 100%|██████████| 27/27 [06:32<00:00, 14.53s/it]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 5/5 [00:45<00:00,  9.07s/it]"]},{"name":"stdout","output_type":"stream","text":["                   all        149        295      0.707      0.116      0.171     0.0959\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"name":"stderr","output_type":"stream","text":["       4/40         0G      1.576      2.353      1.493          5        640: 100%|██████████| 27/27 [06:24<00:00, 14.24s/it]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 5/5 [00:45<00:00,  9.15s/it]"]},{"name":"stdout","output_type":"stream","text":["                   all        149        295      0.719       0.23      0.239      0.144\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"name":"stderr","output_type":"stream","text":["       5/40         0G      1.607      2.159      1.495          5        640: 100%|██████████| 27/27 [06:22<00:00, 14.17s/it]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 5/5 [00:45<00:00,  9.11s/it]"]},{"name":"stdout","output_type":"stream","text":["                   all        149        295      0.599      0.327      0.294      0.192\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"name":"stderr","output_type":"stream","text":["       6/40         0G      1.536      1.982      1.447         12        640: 100%|██████████| 27/27 [06:16<00:00, 13.94s/it]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 5/5 [00:46<00:00,  9.29s/it]"]},{"name":"stdout","output_type":"stream","text":["                   all        149        295      0.534      0.319      0.199      0.118\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"name":"stderr","output_type":"stream","text":["       7/40         0G      1.577      1.931       1.47         14        640: 100%|██████████| 27/27 [06:19<00:00, 14.05s/it]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 5/5 [00:45<00:00,  9.10s/it]"]},{"name":"stdout","output_type":"stream","text":["                   all        149        295      0.716      0.239      0.372      0.238\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"name":"stderr","output_type":"stream","text":["       8/40         0G      1.486      1.832      1.386          5        640: 100%|██████████| 27/27 [06:22<00:00, 14.17s/it]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 5/5 [00:44<00:00,  8.97s/it]"]},{"name":"stdout","output_type":"stream","text":["                   all        149        295      0.594       0.39      0.446      0.258\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"name":"stderr","output_type":"stream","text":["       9/40         0G      1.541      1.785      1.418          9        640: 100%|██████████| 27/27 [06:17<00:00, 13.99s/it]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 5/5 [00:45<00:00,  9.04s/it]"]},{"name":"stdout","output_type":"stream","text":["                   all        149        295      0.699      0.343      0.386      0.251\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"name":"stderr","output_type":"stream","text":["      10/40         0G      1.484      1.771      1.417         11        640: 100%|██████████| 27/27 [06:23<00:00, 14.19s/it]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 5/5 [00:45<00:00,  9.12s/it]"]},{"name":"stdout","output_type":"stream","text":["                   all        149        295       0.85      0.383      0.517      0.337\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"name":"stderr","output_type":"stream","text":["      11/40         0G      1.407      1.588      1.357         12        640: 100%|██████████| 27/27 [06:21<00:00, 14.13s/it]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 5/5 [00:44<00:00,  8.93s/it]"]},{"name":"stdout","output_type":"stream","text":["                   all        149        295      0.825      0.391      0.494      0.313\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"name":"stderr","output_type":"stream","text":["      12/40         0G      1.435       1.55      1.359         15        640: 100%|██████████| 27/27 [06:17<00:00, 13.97s/it]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 5/5 [00:45<00:00,  9.05s/it]"]},{"name":"stdout","output_type":"stream","text":["                   all        149        295      0.811      0.364      0.429      0.282\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"name":"stderr","output_type":"stream","text":["      13/40         0G      1.439      1.523      1.354         10        640: 100%|██████████| 27/27 [06:24<00:00, 14.24s/it]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 5/5 [00:48<00:00,  9.67s/it]"]},{"name":"stdout","output_type":"stream","text":["                   all        149        295      0.882      0.388       0.48      0.284\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"name":"stderr","output_type":"stream","text":["      14/40         0G      1.362      1.512      1.305         13        640: 100%|██████████| 27/27 [06:18<00:00, 14.02s/it]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 5/5 [00:44<00:00,  8.84s/it]"]},{"name":"stdout","output_type":"stream","text":["                   all        149        295      0.531      0.345      0.437      0.303\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"name":"stderr","output_type":"stream","text":["      15/40         0G      1.427      1.559       1.32         21        640: 100%|██████████| 27/27 [06:18<00:00, 14.03s/it]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 5/5 [00:44<00:00,  8.92s/it]"]},{"name":"stdout","output_type":"stream","text":["                   all        149        295      0.767      0.352      0.405      0.278\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"name":"stderr","output_type":"stream","text":["      16/40         0G      1.369      1.434      1.316         25        640: 100%|██████████| 27/27 [06:22<00:00, 14.17s/it]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 5/5 [00:45<00:00,  9.02s/it]"]},{"name":"stdout","output_type":"stream","text":["                   all        149        295      0.519      0.512      0.474      0.318\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"name":"stderr","output_type":"stream","text":["      17/40         0G      1.355       1.36      1.317         11        640: 100%|██████████| 27/27 [06:20<00:00, 14.10s/it]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 5/5 [00:44<00:00,  8.94s/it]"]},{"name":"stdout","output_type":"stream","text":["                   all        149        295      0.793      0.373      0.459      0.313\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"name":"stderr","output_type":"stream","text":["      18/40         0G      1.333       1.34       1.31         14        640: 100%|██████████| 27/27 [06:19<00:00, 14.07s/it]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 5/5 [00:44<00:00,  8.91s/it]"]},{"name":"stdout","output_type":"stream","text":["                   all        149        295      0.515      0.503      0.496      0.315\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"name":"stderr","output_type":"stream","text":["      19/40         0G      1.296      1.272      1.274         20        640: 100%|██████████| 27/27 [06:21<00:00, 14.12s/it]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 5/5 [00:44<00:00,  8.94s/it]"]},{"name":"stdout","output_type":"stream","text":["                   all        149        295      0.834      0.473      0.542      0.345\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"name":"stderr","output_type":"stream","text":["      20/40         0G      1.292      1.261       1.26         13        640: 100%|██████████| 27/27 [06:22<00:00, 14.17s/it]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 5/5 [00:44<00:00,  8.89s/it]"]},{"name":"stdout","output_type":"stream","text":["                   all        149        295      0.899      0.501      0.567      0.357\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"name":"stderr","output_type":"stream","text":["      21/40         0G      1.298      1.252       1.27         13        640: 100%|██████████| 27/27 [06:16<00:00, 13.96s/it]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 5/5 [00:48<00:00,  9.72s/it]"]},{"name":"stdout","output_type":"stream","text":["                   all        149        295      0.789      0.511      0.531      0.327\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"name":"stderr","output_type":"stream","text":["      22/40         0G      1.264      1.208      1.249         18        640: 100%|██████████| 27/27 [06:18<00:00, 14.01s/it]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 5/5 [00:45<00:00,  9.01s/it]"]},{"name":"stdout","output_type":"stream","text":["                   all        149        295       0.66      0.555      0.568      0.342\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"name":"stderr","output_type":"stream","text":["      23/40         0G      1.297      1.265      1.291         17        640: 100%|██████████| 27/27 [06:21<00:00, 14.14s/it]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 5/5 [00:44<00:00,  8.92s/it]"]},{"name":"stdout","output_type":"stream","text":["                   all        149        295      0.861      0.353       0.48      0.321\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"name":"stderr","output_type":"stream","text":["      24/40         0G       1.29       1.18      1.271         21        640: 100%|██████████| 27/27 [06:18<00:00, 14.01s/it]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 5/5 [00:44<00:00,  8.90s/it]"]},{"name":"stdout","output_type":"stream","text":["                   all        149        295      0.902      0.417      0.495      0.344\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"name":"stderr","output_type":"stream","text":["      25/40         0G       1.26      1.184      1.235         13        640: 100%|██████████| 27/27 [06:25<00:00, 14.26s/it]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 5/5 [00:45<00:00,  9.01s/it]"]},{"name":"stdout","output_type":"stream","text":["                   all        149        295      0.812       0.45      0.512      0.386\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"name":"stderr","output_type":"stream","text":["      26/40         0G      1.212      1.109      1.202         10        640: 100%|██████████| 27/27 [06:21<00:00, 14.12s/it]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 5/5 [00:45<00:00,  9.17s/it]"]},{"name":"stdout","output_type":"stream","text":["                   all        149        295      0.552      0.601       0.58      0.375\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"name":"stderr","output_type":"stream","text":["      27/40         0G      1.214      1.115      1.227         17        640: 100%|██████████| 27/27 [06:23<00:00, 14.20s/it]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 5/5 [00:44<00:00,  8.94s/it]"]},{"name":"stdout","output_type":"stream","text":["                   all        149        295      0.617      0.524      0.577      0.353\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"name":"stderr","output_type":"stream","text":["      28/40         0G      1.205      1.067      1.201         11        640: 100%|██████████| 27/27 [06:32<00:00, 14.56s/it]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 5/5 [00:46<00:00,  9.21s/it]"]},{"name":"stdout","output_type":"stream","text":["                   all        149        295       0.76      0.535      0.592      0.398\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"name":"stderr","output_type":"stream","text":["      29/40         0G      1.246      1.074       1.23         16        640: 100%|██████████| 27/27 [06:31<00:00, 14.49s/it]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 5/5 [00:48<00:00,  9.61s/it]"]},{"name":"stdout","output_type":"stream","text":["                   all        149        295      0.639      0.625      0.606      0.403\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"name":"stderr","output_type":"stream","text":["      30/40         0G      1.168      1.007      1.208          7        640: 100%|██████████| 27/27 [06:27<00:00, 14.34s/it]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 5/5 [00:45<00:00,  9.01s/it]"]},{"name":"stdout","output_type":"stream","text":["                   all        149        295      0.647      0.505      0.576      0.365\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["Closing dataloader mosaic\n","\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"name":"stderr","output_type":"stream","text":["      31/40         0G      1.231      1.129      1.214          8        640: 100%|██████████| 27/27 [06:28<00:00, 14.38s/it]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 5/5 [00:45<00:00,  9.12s/it]"]},{"name":"stdout","output_type":"stream","text":["                   all        149        295      0.608      0.628      0.586      0.368\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"name":"stderr","output_type":"stream","text":["      32/40         0G      1.198      1.085       1.21          6        640: 100%|██████████| 27/27 [06:22<00:00, 14.17s/it]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 5/5 [00:49<00:00,  9.80s/it]"]},{"name":"stdout","output_type":"stream","text":["                   all        149        295        0.9      0.457      0.525      0.343\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"name":"stderr","output_type":"stream","text":["      33/40         0G      1.168      1.023      1.201          6        640: 100%|██████████| 27/27 [06:19<00:00, 14.06s/it]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 5/5 [00:44<00:00,  8.92s/it]"]},{"name":"stdout","output_type":"stream","text":["                   all        149        295      0.589      0.581      0.566      0.362\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"name":"stderr","output_type":"stream","text":["      34/40         0G      1.145      1.026      1.177          8        640: 100%|██████████| 27/27 [06:20<00:00, 14.10s/it]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 5/5 [00:45<00:00,  9.13s/it]"]},{"name":"stdout","output_type":"stream","text":["                   all        149        295       0.57      0.616      0.633      0.379\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"name":"stderr","output_type":"stream","text":["      35/40         0G      1.166      1.006      1.173          4        640: 100%|██████████| 27/27 [06:25<00:00, 14.28s/it]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 5/5 [00:45<00:00,  9.17s/it]"]},{"name":"stdout","output_type":"stream","text":["                   all        149        295      0.536      0.682      0.623      0.389\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"name":"stderr","output_type":"stream","text":["      36/40         0G      1.101      0.933      1.138          6        640: 100%|██████████| 27/27 [06:28<00:00, 14.41s/it]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 5/5 [00:48<00:00,  9.76s/it]"]},{"name":"stdout","output_type":"stream","text":["                   all        149        295      0.722       0.54       0.61      0.384\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"name":"stderr","output_type":"stream","text":["      37/40         0G      1.122     0.9471      1.164          8        640: 100%|██████████| 27/27 [06:30<00:00, 14.45s/it]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 5/5 [00:46<00:00,  9.21s/it]"]},{"name":"stdout","output_type":"stream","text":["                   all        149        295      0.531      0.572      0.574      0.386\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"name":"stderr","output_type":"stream","text":["      38/40         0G      1.121      0.952      1.165         23        640: 100%|██████████| 27/27 [06:32<00:00, 14.55s/it]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 5/5 [00:50<00:00, 10.00s/it]"]},{"name":"stdout","output_type":"stream","text":["                   all        149        295      0.591      0.555      0.603      0.381\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"name":"stderr","output_type":"stream","text":["      39/40         0G      1.112     0.9106       1.16          5        640: 100%|██████████| 27/27 [06:27<00:00, 14.34s/it]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 5/5 [00:45<00:00,  9.07s/it]"]},{"name":"stdout","output_type":"stream","text":["                   all        149        295        0.7       0.57      0.591      0.395\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"name":"stderr","output_type":"stream","text":["      40/40         0G        1.1     0.9066      1.154         12        640: 100%|██████████| 27/27 [06:37<00:00, 14.74s/it]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 5/5 [00:45<00:00,  9.19s/it]"]},{"name":"stdout","output_type":"stream","text":["                   all        149        295      0.716      0.568      0.599      0.386\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["\n","40 epochs completed in 4.849 hours.\n","Optimizer stripped from runs/detect/train2/weights/last.pt, 6.2MB\n","Optimizer stripped from runs/detect/train2/weights/best.pt, 6.2MB\n","\n","Validating runs/detect/train2/weights/best.pt...\n","Ultralytics YOLOv8.2.79 🚀 Python-3.10.12 torch-2.3.1+cu121 CPU (Intel Xeon 2.20GHz)\n","Model summary (fused): 168 layers, 3,006,428 parameters, 0 gradients, 8.1 GFLOPs\n"]},{"name":"stderr","output_type":"stream","text":["                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 5/5 [00:43<00:00,  8.70s/it]\n"]},{"name":"stdout","output_type":"stream","text":["                   all        149        295      0.641      0.625      0.606      0.403\n","                 stain        124        266       0.56      0.602      0.597      0.281\n","                  hole         16         20      0.644        0.5      0.506      0.368\n","                  loop          5          5      0.503        0.4      0.327     0.0802\n","                 setup          4          4      0.856          1      0.995      0.884\n","Speed: 2.0ms preprocess, 204.6ms inference, 0.0ms loss, 0.6ms postprocess per image\n","Results saved to \u001b[1mruns/detect/train2\u001b[0m\n"]}],"source":["import os\n","\n","from ultralytics import YOLO\n","\n","\n","# Load a model\n","model = YOLO('yolov8n.pt')  # build a new model from scratch\n","\n","# Use the model\n","results = model.train(data=os.path.join(ROOT_DIR, \"config.yaml\"), epochs=100)  # train the model\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QZWnopkdtc4Z"},"outputs":[],"source":["!scp -r /content/runs '/content/gdrive/My Drive/Model_train/data'\n"]},{"cell_type":"markdown","metadata":{"id":"kp6Yl6C3FFfq"},"source":["# **Image predict**"]},{"cell_type":"code","source":["from google.colab import drive\n","import os\n","import cv2\n","from ultralytics import YOLO\n","from google.colab.patches import cv2_imshow\n","from google.colab import files\n","\n","# Define directories\n","MODEL_PATH = '/content/gdrive/My Drive/Model_train/data/runs/detect/train2/weights/best.pt'\n","OUTPUT_DIR = '/content/gdrive/My Drive/Model_train/data/predict_images'\n","\n","\n","# Load the custom model\n","print(f\"Loading model from {MODEL_PATH}\")\n","model = YOLO(MODEL_PATH)\n","\n","def preprocess_image(image):\n","    # Resize the image to the input size of the model\n","    return cv2.resize(image, (640, 640))\n","\n","def detect_detection(image_path):\n","    # Load the image\n","    image = cv2.imread(image_path)\n","    if image is None:\n","        print(f\"Error: Could not load image from {image_path}. Check if the path is correct.\")\n","        return\n","\n","    # Preprocess the image\n","    image = preprocess_image(image)\n","\n","    # Perform prediction on the entire preprocessed image\n","    results = model(image)[0]\n","\n","    # Annotate the image with predictions\n","    threshold = 0.01  # Lower threshold temporarily\n","    detections = 0\n","    for result in results.boxes.data.tolist():\n","        x1, y1, x2, y2, score, class_id = result\n","        if score > threshold:\n","            detections += 1\n","            cv2.rectangle(image, (int(x1), int(y1)), (int(x2), int(y2)), (0, 255, 0), 2)\n","            cv2.putText(image, results.names[int(class_id)].upper(), (int(x1), int(y1 - 10)),\n","                        cv2.FONT_HERSHEY_SIMPLEX, 1.3, (0, 255, 0), 1, cv2.LINE_AA)\n","\n","    if detections == 0:\n","        print(\"No detections above the threshold.\")\n","    else:\n","        print(f\"Number of detections: {detections}\")\n","\n","    # Save and display the annotated image\n","    output_image_path = os.path.join(OUTPUT_DIR, os.path.basename(image_path).replace('.jpg', '_processed.jpg'))\n","    cv2.imwrite(output_image_path, image)\n","    print(f\"Saved processed image to {output_image_path}\")\n","\n","    # Display the annotated image\n","    cv2_imshow(image)\n","    cv2.waitKey(0)\n","    cv2.destroyAllWindows()\n","\n","# Upload a photo\n","uploaded = files.upload()\n","\n","# Detect stains on the uploaded image\n","for filename in uploaded.keys():\n","    print(f\"Processing {filename}...\")\n","    detect_detection(filename)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1O8Q4STBlcyck-fCgKf-FWnHlw-rjYWB7"},"collapsed":true,"id":"IuCcGC2tewUF","executionInfo":{"status":"ok","timestamp":1724271064683,"user_tz":-360,"elapsed":220365,"user":{"displayName":"Boolean Orbs","userId":"07392357175196698456"}},"outputId":"c5e1561f-6949-4f91-d16c-fbed59d8d4a3"},"execution_count":26,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"markdown","metadata":{"id":"gREtcuDS_E7B"},"source":["## **Video Prediction**"]},{"cell_type":"code","execution_count":28,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"d3raO_nq_EWB","outputId":"1d457d14-71fa-4db0-98d0-fed5f85b1464","executionInfo":{"status":"ok","timestamp":1724271913915,"user_tz":-360,"elapsed":69905,"user":{"displayName":"Boolean Orbs","userId":"07392357175196698456"}}},"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","     <input type=\"file\" id=\"files-c64aceaf-01a4-4fcd-acde-74dbcc7642bd\" name=\"files[]\" multiple disabled\n","        style=\"border:none\" />\n","     <output id=\"result-c64aceaf-01a4-4fcd-acde-74dbcc7642bd\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script>// Copyright 2017 Google LLC\n","//\n","// Licensed under the Apache License, Version 2.0 (the \"License\");\n","// you may not use this file except in compliance with the License.\n","// You may obtain a copy of the License at\n","//\n","//      http://www.apache.org/licenses/LICENSE-2.0\n","//\n","// Unless required by applicable law or agreed to in writing, software\n","// distributed under the License is distributed on an \"AS IS\" BASIS,\n","// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","// See the License for the specific language governing permissions and\n","// limitations under the License.\n","\n","/**\n"," * @fileoverview Helpers for google.colab Python module.\n"," */\n","(function(scope) {\n","function span(text, styleAttributes = {}) {\n","  const element = document.createElement('span');\n","  element.textContent = text;\n","  for (const key of Object.keys(styleAttributes)) {\n","    element.style[key] = styleAttributes[key];\n","  }\n","  return element;\n","}\n","\n","// Max number of bytes which will be uploaded at a time.\n","const MAX_PAYLOAD_SIZE = 100 * 1024;\n","\n","function _uploadFiles(inputId, outputId) {\n","  const steps = uploadFilesStep(inputId, outputId);\n","  const outputElement = document.getElementById(outputId);\n","  // Cache steps on the outputElement to make it available for the next call\n","  // to uploadFilesContinue from Python.\n","  outputElement.steps = steps;\n","\n","  return _uploadFilesContinue(outputId);\n","}\n","\n","// This is roughly an async generator (not supported in the browser yet),\n","// where there are multiple asynchronous steps and the Python side is going\n","// to poll for completion of each step.\n","// This uses a Promise to block the python side on completion of each step,\n","// then passes the result of the previous step as the input to the next step.\n","function _uploadFilesContinue(outputId) {\n","  const outputElement = document.getElementById(outputId);\n","  const steps = outputElement.steps;\n","\n","  const next = steps.next(outputElement.lastPromiseValue);\n","  return Promise.resolve(next.value.promise).then((value) => {\n","    // Cache the last promise value to make it available to the next\n","    // step of the generator.\n","    outputElement.lastPromiseValue = value;\n","    return next.value.response;\n","  });\n","}\n","\n","/**\n"," * Generator function which is called between each async step of the upload\n"," * process.\n"," * @param {string} inputId Element ID of the input file picker element.\n"," * @param {string} outputId Element ID of the output display.\n"," * @return {!Iterable<!Object>} Iterable of next steps.\n"," */\n","function* uploadFilesStep(inputId, outputId) {\n","  const inputElement = document.getElementById(inputId);\n","  inputElement.disabled = false;\n","\n","  const outputElement = document.getElementById(outputId);\n","  outputElement.innerHTML = '';\n","\n","  const pickedPromise = new Promise((resolve) => {\n","    inputElement.addEventListener('change', (e) => {\n","      resolve(e.target.files);\n","    });\n","  });\n","\n","  const cancel = document.createElement('button');\n","  inputElement.parentElement.appendChild(cancel);\n","  cancel.textContent = 'Cancel upload';\n","  const cancelPromise = new Promise((resolve) => {\n","    cancel.onclick = () => {\n","      resolve(null);\n","    };\n","  });\n","\n","  // Wait for the user to pick the files.\n","  const files = yield {\n","    promise: Promise.race([pickedPromise, cancelPromise]),\n","    response: {\n","      action: 'starting',\n","    }\n","  };\n","\n","  cancel.remove();\n","\n","  // Disable the input element since further picks are not allowed.\n","  inputElement.disabled = true;\n","\n","  if (!files) {\n","    return {\n","      response: {\n","        action: 'complete',\n","      }\n","    };\n","  }\n","\n","  for (const file of files) {\n","    const li = document.createElement('li');\n","    li.append(span(file.name, {fontWeight: 'bold'}));\n","    li.append(span(\n","        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n","        `last modified: ${\n","            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n","                                    'n/a'} - `));\n","    const percent = span('0% done');\n","    li.appendChild(percent);\n","\n","    outputElement.appendChild(li);\n","\n","    const fileDataPromise = new Promise((resolve) => {\n","      const reader = new FileReader();\n","      reader.onload = (e) => {\n","        resolve(e.target.result);\n","      };\n","      reader.readAsArrayBuffer(file);\n","    });\n","    // Wait for the data to be ready.\n","    let fileData = yield {\n","      promise: fileDataPromise,\n","      response: {\n","        action: 'continue',\n","      }\n","    };\n","\n","    // Use a chunked sending to avoid message size limits. See b/62115660.\n","    let position = 0;\n","    do {\n","      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n","      const chunk = new Uint8Array(fileData, position, length);\n","      position += length;\n","\n","      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n","      yield {\n","        response: {\n","          action: 'append',\n","          file: file.name,\n","          data: base64,\n","        },\n","      };\n","\n","      let percentDone = fileData.byteLength === 0 ?\n","          100 :\n","          Math.round((position / fileData.byteLength) * 100);\n","      percent.textContent = `${percentDone}% done`;\n","\n","    } while (position < fileData.byteLength);\n","  }\n","\n","  // All done.\n","  yield {\n","    response: {\n","      action: 'complete',\n","    }\n","  };\n","}\n","\n","scope.google = scope.google || {};\n","scope.google.colab = scope.google.colab || {};\n","scope.google.colab._files = {\n","  _uploadFiles,\n","  _uploadFilesContinue,\n","};\n","})(self);\n","</script> "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Saving WhatsApp Video 2024-07-04 at 1.10.39 AM.mp4 to WhatsApp Video 2024-07-04 at 1.10.39 AM.mp4\n","\n","0: 384x640 (no detections), 144.0ms\n","Speed: 4.5ms preprocess, 144.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 (no detections), 133.7ms\n","Speed: 4.1ms preprocess, 133.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 (no detections), 139.6ms\n","Speed: 3.9ms preprocess, 139.6ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 (no detections), 132.3ms\n","Speed: 5.6ms preprocess, 132.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 (no detections), 135.8ms\n","Speed: 6.1ms preprocess, 135.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 hole, 140.3ms\n","Speed: 5.9ms preprocess, 140.3ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 hole, 134.5ms\n","Speed: 5.8ms preprocess, 134.5ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 (no detections), 137.0ms\n","Speed: 4.9ms preprocess, 137.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 stain, 138.8ms\n","Speed: 4.2ms preprocess, 138.8ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 stain, 143.7ms\n","Speed: 8.5ms preprocess, 143.7ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 stain, 137.8ms\n","Speed: 5.3ms preprocess, 137.8ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 stains, 135.6ms\n","Speed: 5.3ms preprocess, 135.6ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 stains, 138.4ms\n","Speed: 6.1ms preprocess, 138.4ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 stains, 138.1ms\n","Speed: 4.3ms preprocess, 138.1ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 stains, 156.9ms\n","Speed: 7.1ms preprocess, 156.9ms inference, 4.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 stains, 144.8ms\n","Speed: 9.0ms preprocess, 144.8ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 stains, 137.4ms\n","Speed: 3.4ms preprocess, 137.4ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 stains, 140.1ms\n","Speed: 5.7ms preprocess, 140.1ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 stains, 134.5ms\n","Speed: 5.4ms preprocess, 134.5ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 (no detections), 144.1ms\n","Speed: 5.8ms preprocess, 144.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 stain, 164.5ms\n","Speed: 6.2ms preprocess, 164.5ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 stains, 139.4ms\n","Speed: 4.7ms preprocess, 139.4ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 stains, 1 hole, 139.3ms\n","Speed: 5.9ms preprocess, 139.3ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 stain, 1 hole, 140.8ms\n","Speed: 6.0ms preprocess, 140.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 stain, 140.5ms\n","Speed: 6.0ms preprocess, 140.5ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 stain, 137.4ms\n","Speed: 5.6ms preprocess, 137.4ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 stain, 165.3ms\n","Speed: 7.7ms preprocess, 165.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 stain, 134.6ms\n","Speed: 5.7ms preprocess, 134.6ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 stains, 1 hole, 136.4ms\n","Speed: 5.5ms preprocess, 136.4ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 stains, 144.3ms\n","Speed: 5.3ms preprocess, 144.3ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 stains, 135.5ms\n","Speed: 6.5ms preprocess, 135.5ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 stain, 135.6ms\n","Speed: 5.8ms preprocess, 135.6ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 stain, 159.6ms\n","Speed: 5.5ms preprocess, 159.6ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 stain, 136.3ms\n","Speed: 5.3ms preprocess, 136.3ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 stain, 199.9ms\n","Speed: 5.4ms preprocess, 199.9ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 stain, 208.4ms\n","Speed: 5.8ms preprocess, 208.4ms inference, 3.9ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 stain, 225.2ms\n","Speed: 5.4ms preprocess, 225.2ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 stains, 212.4ms\n","Speed: 6.6ms preprocess, 212.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 stain, 208.3ms\n","Speed: 6.9ms preprocess, 208.3ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 stain, 212.6ms\n","Speed: 5.7ms preprocess, 212.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 stain, 211.0ms\n","Speed: 6.1ms preprocess, 211.0ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 stains, 216.8ms\n","Speed: 5.6ms preprocess, 216.8ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 stains, 209.7ms\n","Speed: 5.4ms preprocess, 209.7ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 stains, 222.0ms\n","Speed: 8.2ms preprocess, 222.0ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 stains, 204.7ms\n","Speed: 6.8ms preprocess, 204.7ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 stains, 222.3ms\n","Speed: 7.9ms preprocess, 222.3ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 stains, 219.9ms\n","Speed: 5.6ms preprocess, 219.9ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 stain, 216.6ms\n","Speed: 6.8ms preprocess, 216.6ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 stain, 239.2ms\n","Speed: 6.7ms preprocess, 239.2ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 stains, 239.6ms\n","Speed: 7.6ms preprocess, 239.6ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 stain, 204.6ms\n","Speed: 6.6ms preprocess, 204.6ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 stain, 205.7ms\n","Speed: 6.4ms preprocess, 205.7ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 stain, 142.5ms\n","Speed: 5.2ms preprocess, 142.5ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 stain, 135.2ms\n","Speed: 5.5ms preprocess, 135.2ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 stains, 146.8ms\n","Speed: 5.4ms preprocess, 146.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 stain, 137.7ms\n","Speed: 5.8ms preprocess, 137.7ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 stains, 136.1ms\n","Speed: 5.9ms preprocess, 136.1ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 stain, 133.6ms\n","Speed: 5.5ms preprocess, 133.6ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 stains, 161.4ms\n","Speed: 5.5ms preprocess, 161.4ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 stain, 138.3ms\n","Speed: 5.0ms preprocess, 138.3ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 stain, 133.6ms\n","Speed: 5.1ms preprocess, 133.6ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 stain, 151.3ms\n","Speed: 5.3ms preprocess, 151.3ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 stain, 132.3ms\n","Speed: 7.5ms preprocess, 132.3ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 (no detections), 130.6ms\n","Speed: 7.0ms preprocess, 130.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 stains, 151.1ms\n","Speed: 5.5ms preprocess, 151.1ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 stain, 131.0ms\n","Speed: 5.8ms preprocess, 131.0ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 stains, 138.3ms\n","Speed: 5.5ms preprocess, 138.3ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 stains, 153.8ms\n","Speed: 6.0ms preprocess, 153.8ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 stains, 140.7ms\n","Speed: 8.8ms preprocess, 140.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 stains, 135.4ms\n","Speed: 6.3ms preprocess, 135.4ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 stains, 146.8ms\n","Speed: 5.0ms preprocess, 146.8ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 stains, 144.4ms\n","Speed: 5.9ms preprocess, 144.4ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 stain, 138.5ms\n","Speed: 5.8ms preprocess, 138.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 stain, 143.9ms\n","Speed: 5.6ms preprocess, 143.9ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 stains, 137.7ms\n","Speed: 6.1ms preprocess, 137.7ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 stains, 142.5ms\n","Speed: 6.0ms preprocess, 142.5ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 stains, 152.2ms\n","Speed: 6.1ms preprocess, 152.2ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 stain, 136.3ms\n","Speed: 5.8ms preprocess, 136.3ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 stains, 131.7ms\n","Speed: 6.0ms preprocess, 131.7ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 stains, 152.1ms\n","Speed: 4.1ms preprocess, 152.1ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 stain, 131.6ms\n","Speed: 5.6ms preprocess, 131.6ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 stain, 146.7ms\n","Speed: 5.3ms preprocess, 146.7ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 (no detections), 136.7ms\n","Speed: 3.8ms preprocess, 136.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 (no detections), 140.3ms\n","Speed: 5.5ms preprocess, 140.3ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 stain, 143.3ms\n","Speed: 5.5ms preprocess, 143.3ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 stain, 148.1ms\n","Speed: 5.1ms preprocess, 148.1ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 stain, 139.9ms\n","Speed: 4.6ms preprocess, 139.9ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 stains, 163.0ms\n","Speed: 6.8ms preprocess, 163.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 stains, 140.6ms\n","Speed: 5.4ms preprocess, 140.6ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 (no detections), 143.8ms\n","Speed: 5.3ms preprocess, 143.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 stain, 143.9ms\n","Speed: 6.4ms preprocess, 143.9ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 (no detections), 140.5ms\n","Speed: 5.5ms preprocess, 140.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 (no detections), 137.1ms\n","Speed: 5.8ms preprocess, 137.1ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 stain, 152.7ms\n","Speed: 7.2ms preprocess, 152.7ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 stains, 133.8ms\n","Speed: 4.0ms preprocess, 133.8ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 stain, 134.2ms\n","Speed: 5.3ms preprocess, 134.2ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 stains, 135.9ms\n","Speed: 5.6ms preprocess, 135.9ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 stains, 145.1ms\n","Speed: 5.3ms preprocess, 145.1ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 (no detections), 142.2ms\n","Speed: 5.9ms preprocess, 142.2ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 stain, 151.9ms\n","Speed: 7.4ms preprocess, 151.9ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 stains, 145.4ms\n","Speed: 5.5ms preprocess, 145.4ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 stains, 138.7ms\n","Speed: 8.9ms preprocess, 138.7ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 stain, 144.3ms\n","Speed: 5.3ms preprocess, 144.3ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 stains, 138.3ms\n","Speed: 5.5ms preprocess, 138.3ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 (no detections), 153.7ms\n","Speed: 6.5ms preprocess, 153.7ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 (no detections), 138.6ms\n","Speed: 6.0ms preprocess, 138.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 stain, 136.3ms\n","Speed: 5.4ms preprocess, 136.3ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 stains, 136.7ms\n","Speed: 5.7ms preprocess, 136.7ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 (no detections), 132.5ms\n","Speed: 5.2ms preprocess, 132.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 stains, 139.5ms\n","Speed: 9.1ms preprocess, 139.5ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 (no detections), 210.4ms\n","Speed: 5.5ms preprocess, 210.4ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 stain, 203.7ms\n","Speed: 6.5ms preprocess, 203.7ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 stain, 212.3ms\n","Speed: 7.4ms preprocess, 212.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 (no detections), 210.0ms\n","Speed: 8.2ms preprocess, 210.0ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 stain, 202.2ms\n","Speed: 6.7ms preprocess, 202.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 (no detections), 212.6ms\n","Speed: 10.4ms preprocess, 212.6ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 stain, 218.5ms\n","Speed: 7.0ms preprocess, 218.5ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 (no detections), 221.0ms\n","Speed: 5.8ms preprocess, 221.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 (no detections), 206.2ms\n","Speed: 5.3ms preprocess, 206.2ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 stain, 207.9ms\n","Speed: 5.2ms preprocess, 207.9ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 stain, 216.8ms\n","Speed: 17.3ms preprocess, 216.8ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 (no detections), 217.0ms\n","Speed: 5.8ms preprocess, 217.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 stain, 230.4ms\n","Speed: 7.5ms preprocess, 230.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 stain, 229.1ms\n","Speed: 5.4ms preprocess, 229.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 (no detections), 222.4ms\n","Speed: 8.4ms preprocess, 222.4ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 (no detections), 230.3ms\n","Speed: 5.5ms preprocess, 230.3ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 stain, 219.0ms\n","Speed: 8.4ms preprocess, 219.0ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 stain, 227.4ms\n","Speed: 8.3ms preprocess, 227.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 stain, 154.8ms\n","Speed: 8.5ms preprocess, 154.8ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 (no detections), 142.7ms\n","Speed: 6.2ms preprocess, 142.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 (no detections), 167.7ms\n","Speed: 6.9ms preprocess, 167.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 (no detections), 141.6ms\n","Speed: 6.2ms preprocess, 141.6ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 stains, 146.1ms\n","Speed: 5.7ms preprocess, 146.1ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 stains, 153.2ms\n","Speed: 5.9ms preprocess, 153.2ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 stain, 145.8ms\n","Speed: 5.2ms preprocess, 145.8ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 stain, 149.4ms\n","Speed: 5.1ms preprocess, 149.4ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 stain, 163.6ms\n","Speed: 5.4ms preprocess, 163.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 (no detections), 143.1ms\n","Speed: 6.4ms preprocess, 143.1ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 (no detections), 152.0ms\n","Speed: 7.6ms preprocess, 152.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 (no detections), 140.7ms\n","Speed: 8.3ms preprocess, 140.7ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 stains, 144.4ms\n","Speed: 5.5ms preprocess, 144.4ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 stains, 168.2ms\n","Speed: 6.9ms preprocess, 168.2ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 stains, 146.9ms\n","Speed: 6.1ms preprocess, 146.9ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 stain, 153.9ms\n","Speed: 5.6ms preprocess, 153.9ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 (no detections), 152.7ms\n","Speed: 4.9ms preprocess, 152.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 stains, 141.1ms\n","Speed: 6.1ms preprocess, 141.1ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 stains, 155.5ms\n","Speed: 4.7ms preprocess, 155.5ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 stains, 164.4ms\n","Speed: 6.0ms preprocess, 164.4ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 stains, 147.0ms\n","Speed: 5.9ms preprocess, 147.0ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 stains, 145.4ms\n","Speed: 6.0ms preprocess, 145.4ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 stains, 139.2ms\n","Speed: 4.7ms preprocess, 139.2ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 stains, 138.3ms\n","Speed: 6.5ms preprocess, 138.3ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 stains, 154.7ms\n","Speed: 6.5ms preprocess, 154.7ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 stains, 152.5ms\n","Speed: 5.9ms preprocess, 152.5ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 stain, 140.1ms\n","Speed: 6.0ms preprocess, 140.1ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 stain, 148.7ms\n","Speed: 6.8ms preprocess, 148.7ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 stain, 137.0ms\n","Speed: 8.0ms preprocess, 137.0ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 stains, 137.5ms\n","Speed: 5.3ms preprocess, 137.5ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 stains, 154.8ms\n","Speed: 5.4ms preprocess, 154.8ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 stains, 147.3ms\n","Speed: 5.1ms preprocess, 147.3ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 stains, 144.4ms\n","Speed: 5.6ms preprocess, 144.4ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 stain, 1 hole, 141.8ms\n","Speed: 6.6ms preprocess, 141.8ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 stain, 138.6ms\n","Speed: 5.1ms preprocess, 138.6ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 stains, 142.1ms\n","Speed: 5.0ms preprocess, 142.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 stain, 146.1ms\n","Speed: 9.2ms preprocess, 146.1ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 stain, 144.2ms\n","Speed: 6.7ms preprocess, 144.2ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 stain, 145.7ms\n","Speed: 5.2ms preprocess, 145.7ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 stain, 137.0ms\n","Speed: 4.7ms preprocess, 137.0ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 stain, 137.4ms\n","Speed: 5.1ms preprocess, 137.4ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 stain, 150.3ms\n","Speed: 5.4ms preprocess, 150.3ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 stains, 141.0ms\n","Speed: 5.8ms preprocess, 141.0ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 stains, 145.6ms\n","Speed: 6.6ms preprocess, 145.6ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 stains, 138.0ms\n","Speed: 6.6ms preprocess, 138.0ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 stains, 145.7ms\n","Speed: 5.0ms preprocess, 145.7ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 stains, 140.0ms\n","Speed: 4.2ms preprocess, 140.0ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 stains, 154.4ms\n","Speed: 6.4ms preprocess, 154.4ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 stains, 134.4ms\n","Speed: 4.4ms preprocess, 134.4ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 stain, 146.2ms\n","Speed: 4.7ms preprocess, 146.2ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 stain, 151.5ms\n","Speed: 6.0ms preprocess, 151.5ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n","Processed video saved at: /content/gdrive/My Drive/Model_train/data/videos/WhatsApp Video 2024-07-04 at 1.10.39 AM_output.mp4\n"]}],"source":["import cv2\n","import os\n","from ultralytics import YOLO\n","from google.colab import files\n","\n","# Load the model\n","model = YOLO('/content/gdrive/My Drive/Model_train/data/runs/detect/train2/weights/last.pt')\n","\n","# Upload video\n","uploaded = files.upload()\n","\n","\n","output_dir = '/content/gdrive/My Drive/Model_train/data/videos'\n","\n","\n","def process_video(video_path):\n","    cap = cv2.VideoCapture(video_path)\n","\n","    # Set the output path to save the processed video in Google Drive\n","    out_path = os.path.join(output_dir, os.path.basename(video_path).replace('.mp4', '_output.mp4'))\n","    out = None\n","\n","    while True:\n","        ret, frame = cap.read()\n","        if not ret:\n","            break\n","\n","        if out is None:\n","            height, width = frame.shape[:2]\n","            fps = int(cap.get(cv2.CAP_PROP_FPS))\n","            out = cv2.VideoWriter(out_path, cv2.VideoWriter_fourcc(*'MP4V'), fps, (width, height))\n","\n","        results = model(frame)[0]\n","        for result in results.boxes.data.tolist():\n","            x1, y1, x2, y2, score, class_id = result\n","            if score > 0.01:\n","                cv2.rectangle(frame, (int(x1), int(y1)), (int(x2), int(y2)), (0, 255, 0), 2)\n","                cv2.putText(frame, results.names[int(class_id)], (int(x1), int(y1 - 10)),\n","                            cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\n","\n","        out.write(frame)\n","\n","    cap.release()\n","    out.release()\n","    print(f\"Processed video saved at: {out_path}\")\n","\n","# Process uploaded video\n","for video in uploaded.keys():\n","    process_video(video)\n"]}],"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNTn58Jn1BjCCsOBVmSc61e"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}